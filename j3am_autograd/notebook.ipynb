{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-01",
   "metadata": {},
   "source": [
    "# Micro-Autograd — Rétropropagation depuis Zéro\n",
    "\n",
    "Implémentation d'un moteur de différentiation automatique scalaire, inspiré de [micrograd](https://github.com/karpathy/micrograd) (A. Karpathy).\n",
    "\n",
    "## Objectifs\n",
    "- Implémenter la classe `Value` avec typage complet\n",
    "- Supporter : `+`, `-`, `*`, `/`, `**`, `relu`, `exp`, `log`\n",
    "- Vérifier les gradients par différences finies (gradient check)\n",
    "- Simuler un neurone complet (forward + backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T13:24:39.083948Z",
     "iopub.status.busy": "2026-02-27T13:24:39.083752Z",
     "iopub.status.idle": "2026-02-27T13:24:39.089826Z",
     "shell.execute_reply": "2026-02-27T13:24:39.089105Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "from typing import Callable, Union\n",
    "\n",
    "Number = Union[int, float]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-03",
   "metadata": {},
   "source": [
    "## 1. La classe `Value`\n",
    "\n",
    "Encapsule un scalaire et construit le graphe de calcul au fur et à mesure.\n",
    "Chaque nœud stocke :\n",
    "- `data` : valeur numérique\n",
    "- `grad` : gradient accumulé (init. à `0.0`)\n",
    "- `_prev` : nœuds parents (pour remonter le graphe)\n",
    "- `_backward` : fermeture calculant le gradient local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T13:24:39.092795Z",
     "iopub.status.busy": "2026-02-27T13:24:39.092554Z",
     "iopub.status.idle": "2026-02-27T13:24:39.102030Z",
     "shell.execute_reply": "2026-02-27T13:24:39.101550Z"
    }
   },
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\"Scalaire avec différentiation automatique (reverse mode).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: float,\n",
    "        _children: tuple[Value, ...] = (),\n",
    "        _op: str = \"\",\n",
    "    ) -> None:\n",
    "        self.data: float = float(data)\n",
    "        self.grad: float = 0.0\n",
    "        self._prev: set[Value] = set(_children)\n",
    "        self._op: str = _op\n",
    "        self._backward: Callable[[], None] = lambda: None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Opérations arithmétiques\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def __add__(self, other: Value | Number) -> Value:\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad  += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other: Number) -> Value:          # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __neg__(self) -> Value:\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other: Value | Number) -> Value:\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other: Number) -> Value:          # other - self\n",
    "        return Value(other) + (-self)\n",
    "\n",
    "    def __mul__(self, other: Value | Number) -> Value:\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad  += other.data * out.grad\n",
    "            other.grad += self.data  * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other: Number) -> Value:          # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __pow__(self, exponent: int | float) -> Value:\n",
    "        assert isinstance(exponent, (int, float)), \"exposant doit être un scalaire\"\n",
    "        out = Value(self.data ** exponent, (self,), f\"**{exponent}\")\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += exponent * (self.data ** (exponent - 1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other: Value | Number) -> Value:\n",
    "        other_val = other if isinstance(other, Value) else Value(other)\n",
    "        return self * other_val ** -1\n",
    "\n",
    "    def __rtruediv__(self, other: Number) -> Value:      # other / self\n",
    "        return Value(other) * self ** -1\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Fonctions d'activation et mathématiques\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def relu(self) -> Value:\n",
    "        out = Value(max(0.0, self.data), (self,), \"ReLU\")\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def exp(self) -> Value:\n",
    "        val = math.exp(self.data)\n",
    "        out = Value(val, (self,), \"exp\")\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += val * out.grad      # d/dx exp(x) = exp(x)\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def log(self) -> Value:\n",
    "        assert self.data > 0, f\"log non défini pour data={self.data:.4f}\"\n",
    "        out = Value(math.log(self.data), (self,), \"log\")\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += (1.0 / self.data) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Backward pass\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        \"\"\"Rétropropagation par tri topologique (reverse mode).\"\"\"\n",
    "        topo: list[Value] = []\n",
    "        visited: set[Value] = set()\n",
    "\n",
    "        def build_topo(v: Value) -> None:\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-05",
   "metadata": {},
   "source": [
    "## 2. Forward Pass — Tests des opérations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T13:24:39.103327Z",
     "iopub.status.busy": "2026-02-27T13:24:39.103221Z",
     "iopub.status.idle": "2026-02-27T13:24:39.107173Z",
     "shell.execute_reply": "2026-02-27T13:24:39.106725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y   = Value(data=-1.0000, grad=0.0000)\n",
      "x + 3   = Value(data=5.0000, grad=0.0000)\n",
      "x * y   = Value(data=-6.0000, grad=0.0000)\n",
      "-x      = Value(data=-2.0000, grad=0.0000)\n",
      "x - y   = Value(data=5.0000, grad=0.0000)\n",
      "x / -4  = Value(data=-0.5000, grad=0.0000)\n",
      "x**3    = Value(data=8.0000, grad=0.0000)\n"
     ]
    }
   ],
   "source": [
    "x = Value(2.0)\n",
    "y = Value(-3.0)\n",
    "\n",
    "# Addition\n",
    "z = x + y\n",
    "assert z.data == -1.0\n",
    "print(f\"x + y   = {z}\")\n",
    "\n",
    "# Scalaire à droite\n",
    "w = x + 3\n",
    "assert w.data == 5.0\n",
    "print(f\"x + 3   = {w}\")\n",
    "\n",
    "# Multiplication\n",
    "p = x * y\n",
    "assert p.data == -6.0\n",
    "print(f\"x * y   = {p}\")\n",
    "\n",
    "# Négation\n",
    "print(f\"-x      = {-x}\")\n",
    "\n",
    "# Soustraction\n",
    "s = x - y\n",
    "assert s.data == 5.0\n",
    "print(f\"x - y   = {s}\")\n",
    "\n",
    "# Division\n",
    "d = x / Value(-4.0)\n",
    "assert abs(d.data - (-0.5)) < 1e-9\n",
    "print(f\"x / -4  = {d}\")\n",
    "\n",
    "# Puissance\n",
    "pw = x ** 3\n",
    "assert pw.data == 8.0\n",
    "print(f\"x**3    = {pw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T13:24:39.108250Z",
     "iopub.status.busy": "2026-02-27T13:24:39.108144Z",
     "iopub.status.idle": "2026-02-27T13:24:39.111098Z",
     "shell.execute_reply": "2026-02-27T13:24:39.110666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu( 3.0) = 3.0\n",
      "relu(-2.0) = 0.0\n",
      "exp(1.0)   = 2.718282\n",
      "log(e)     = 1.000000\n"
     ]
    }
   ],
   "source": [
    "# ReLU\n",
    "assert Value( 3.0).relu().data == 3.0\n",
    "assert Value(-2.0).relu().data == 0.0\n",
    "print(f\"relu( 3.0) = {Value( 3.0).relu().data}\")\n",
    "print(f\"relu(-2.0) = {Value(-2.0).relu().data}\")\n",
    "\n",
    "# Exponentielle\n",
    "e_val = Value(1.0).exp()\n",
    "assert abs(e_val.data - math.e) < 1e-10\n",
    "print(f\"exp(1.0)   = {e_val.data:.6f}\")\n",
    "\n",
    "# Logarithme\n",
    "l_val = Value(math.e).log()\n",
    "assert abs(l_val.data - 1.0) < 1e-10\n",
    "print(f\"log(e)     = {l_val.data:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-08",
   "metadata": {},
   "source": [
    "## 3. Backward Pass — Calcul des gradients\n",
    "\n",
    "Le backward pass utilise un **tri topologique** du graphe de calcul pour propager les gradients de la sortie vers les entrées.\n",
    "\n",
    "**Règle générale :** `node.grad += gradient_amont × dérivée_locale`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T13:24:39.112042Z",
     "iopub.status.busy": "2026-02-27T13:24:39.111945Z",
     "iopub.status.idle": "2026-02-27T13:24:39.114852Z",
     "shell.execute_reply": "2026-02-27T13:24:39.114350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = x*y + x       = -4.0\n",
      "dz/dx = -2.0   (attendu : -2.0)\n",
      "dz/dy = 2.0    (attendu : 2.0)\n"
     ]
    }
   ],
   "source": [
    "# f(x, y) = x * y + x\n",
    "# df/dx = y + 1 = -3 + 1 = -2\n",
    "# df/dy = x = 2\n",
    "x = Value(2.0)\n",
    "y = Value(-3.0)\n",
    "z = x * y + x\n",
    "z.backward()\n",
    "\n",
    "print(f\"z = x*y + x       = {z.data}\")\n",
    "print(f\"dz/dx = {x.grad:.1f}   (attendu : {float(-3 + 1):.1f})\")\n",
    "print(f\"dz/dy = {y.grad:.1f}    (attendu : {2.0:.1f})\")\n",
    "assert abs(x.grad - (-2.0)) < 1e-9\n",
    "assert abs(y.grad -   2.0)  < 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T13:24:39.115871Z",
     "iopub.status.busy": "2026-02-27T13:24:39.115774Z",
     "iopub.status.idle": "2026-02-27T13:24:39.118450Z",
     "shell.execute_reply": "2026-02-27T13:24:39.117984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = relu(x²-4) = 5.0\n",
      "dy/dx = 6.0   (attendu : 6.0)\n"
     ]
    }
   ],
   "source": [
    "# f(x) = relu(x^2 - 4)  à x = 3\n",
    "# x^2 = 9  =>  9 - 4 = 5  =>  relu(5) = 5\n",
    "# df/dx = relu'(5) * 2x = 1 * 6 = 6\n",
    "x = Value(3.0)\n",
    "y = (x ** 2 - 4.0).relu()\n",
    "y.backward()\n",
    "\n",
    "print(f\"y = relu(x²-4) = {y.data}\")\n",
    "print(f\"dy/dx = {x.grad:.1f}   (attendu : {2 * 3.0:.1f})\")\n",
    "assert abs(x.grad - 6.0) < 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Vérification Numérique des Gradients (Gradient Check)\n",
    "\n",
    "La **différence finie centrée** estime le gradient numériquement :\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(\\ldots,\\, x_i + \\varepsilon,\\, \\ldots) - f(\\ldots,\\, x_i - \\varepsilon,\\, \\ldots)}{2\\varepsilon}$$\n",
    "\n",
    "On compare cette estimation au gradient calculé par autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T13:24:39.119379Z",
     "iopub.status.busy": "2026-02-27T13:24:39.119294Z",
     "iopub.status.idle": "2026-02-27T13:24:39.122665Z",
     "shell.execute_reply": "2026-02-27T13:24:39.122274Z"
    }
   },
   "outputs": [],
   "source": [
    "def grad_check(\n",
    "    f: Callable[..., Value],\n",
    "    inputs: list[float],\n",
    "    eps: float = 1e-5,\n",
    "    tol: float = 1e-4,\n",
    ") -> None:\n",
    "    \"\"\"Vérifie les gradients autograd par différences finies centrées.\"\"\"\n",
    "    vals = [Value(x) for x in inputs]\n",
    "    out = f(*vals)\n",
    "    out.backward()\n",
    "    ag = [v.grad for v in vals]\n",
    "\n",
    "    print(f\"  {'var':<6} {'autograd':>12} {'numérique':>12} {'ok':>4}\")\n",
    "    print(f\"  {'-' * 38}\")\n",
    "    all_ok = True\n",
    "    for i, (a, inp) in enumerate(zip(ag, inputs)):\n",
    "        inp_p = inputs.copy(); inp_p[i] += eps\n",
    "        inp_m = inputs.copy(); inp_m[i] -= eps\n",
    "        ng = (\n",
    "            f(*[Value(x) for x in inp_p]).data\n",
    "          - f(*[Value(x) for x in inp_m]).data\n",
    "        ) / (2 * eps)\n",
    "        ok = abs(a - ng) < tol\n",
    "        all_ok = all_ok and ok\n",
    "        status = \"✓\" if ok else \"✗\"\n",
    "        print(f\"  x_{i:<4} {a:>12.6f} {ng:>12.6f} {status:>4}\")\n",
    "    print()\n",
    "    assert all_ok, \"Gradient check échoué !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T13:24:39.123507Z",
     "iopub.status.busy": "2026-02-27T13:24:39.123423Z",
     "iopub.status.idle": "2026-02-27T13:24:39.126866Z",
     "shell.execute_reply": "2026-02-27T13:24:39.126508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x, y) = x * y\n",
      "  var        autograd    numérique   ok\n",
      "  --------------------------------------\n",
      "  x_0       -3.000000    -3.000000    ✓\n",
      "  x_1        2.000000     2.000000    ✓\n",
      "\n",
      "f(x) = x ** 3\n",
      "  var        autograd    numérique   ok\n",
      "  --------------------------------------\n",
      "  x_0       12.000000    12.000000    ✓\n",
      "\n",
      "f(x) = exp(x)\n",
      "  var        autograd    numérique   ok\n",
      "  --------------------------------------\n",
      "  x_0        2.718282     2.718282    ✓\n",
      "\n",
      "f(x) = log(x)\n",
      "  var        autograd    numérique   ok\n",
      "  --------------------------------------\n",
      "  x_0        0.500000     0.500000    ✓\n",
      "\n",
      "f(x) = relu(x)  [x > 0]\n",
      "  var        autograd    numérique   ok\n",
      "  --------------------------------------\n",
      "  x_0        1.000000     1.000000    ✓\n",
      "\n",
      "f(x) = relu(x)  [x < 0]\n",
      "  var        autograd    numérique   ok\n",
      "  --------------------------------------\n",
      "  x_0        0.000000     0.000000    ✓\n",
      "\n",
      "f(x, y) = relu(x * y + x)\n",
      "  var        autograd    numérique   ok\n",
      "  --------------------------------------\n",
      "  x_0        2.000000     2.000000    ✓\n",
      "  x_1        1.000000     1.000000    ✓\n",
      "\n",
      "Tous les gradients sont corrects ✓\n"
     ]
    }
   ],
   "source": [
    "print(\"f(x, y) = x * y\")\n",
    "grad_check(lambda x, y: x * y, [2.0, -3.0])\n",
    "\n",
    "print(\"f(x) = x ** 3\")\n",
    "grad_check(lambda x: x ** 3, [2.0])\n",
    "\n",
    "print(\"f(x) = exp(x)\")\n",
    "grad_check(lambda x: x.exp(), [1.0])\n",
    "\n",
    "print(\"f(x) = log(x)\")\n",
    "grad_check(lambda x: x.log(), [2.0])\n",
    "\n",
    "print(\"f(x) = relu(x)  [x > 0]\")\n",
    "grad_check(lambda x: x.relu(), [1.5])\n",
    "\n",
    "print(\"f(x) = relu(x)  [x < 0]\")\n",
    "grad_check(lambda x: x.relu(), [-1.5])\n",
    "\n",
    "print(\"f(x, y) = relu(x * y + x)\")\n",
    "grad_check(lambda x, y: (x * y + x).relu(), [1.0, 1.0])\n",
    "\n",
    "print(\"Tous les gradients sont corrects ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Exemple Complet — Un Neurone\n",
    "\n",
    "On simule un neurone ReLU à deux entrées :\n",
    "\n",
    "$$\\hat{y} = \\text{ReLU}(w_1 x_1 + w_2 x_2 + b)$$\n",
    "\n",
    "et on calcule le gradient de la sortie par rapport à chaque paramètre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T13:24:39.127777Z",
     "iopub.status.busy": "2026-02-27T13:24:39.127692Z",
     "iopub.status.idle": "2026-02-27T13:24:39.131034Z",
     "shell.execute_reply": "2026-02-27T13:24:39.130664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "── Forward Pass ───────────────────────────────\n",
      "  w1*x1 + w2*x2 + b  = 0.5000\n",
      "  ReLU(pré-activation) = 0.5000\n",
      "\n",
      "── Gradients ∂out/∂· ──────────────────────────\n",
      "  ∂/∂w1 = 2.0000   (= x1 = 2.0)\n",
      "  ∂/∂w2 = 3.0000   (= x2 = 3.0)\n",
      "  ∂/∂b  = 1.0000    (= 1)\n",
      "  ∂/∂x1 = 0.5000   (= w1 = 0.5)\n",
      "  ∂/∂x2 = -0.5000  (= w2 = -0.5)\n"
     ]
    }
   ],
   "source": [
    "# Entrées\n",
    "x1 = Value(2.0)\n",
    "x2 = Value(3.0)\n",
    "\n",
    "# Paramètres (à apprendre par gradient descent)\n",
    "w1 = Value( 0.5)\n",
    "w2 = Value(-0.5)\n",
    "b  = Value( 1.0)\n",
    "\n",
    "# ── Forward ───────────────────────────────────────────────────────────\n",
    "pre_act = w1 * x1 + w2 * x2 + b   # 0.5*2 + (-0.5)*3 + 1.0 = 0.5\n",
    "out     = pre_act.relu()           # ReLU(0.5) = 0.5\n",
    "\n",
    "print(\"── Forward Pass ───────────────────────────────\")\n",
    "print(f\"  w1*x1 + w2*x2 + b  = {pre_act.data:.4f}\")\n",
    "print(f\"  ReLU(pré-activation) = {out.data:.4f}\")\n",
    "\n",
    "# ── Backward ──────────────────────────────────────────────────────────\n",
    "out.backward()\n",
    "\n",
    "print()\n",
    "print(\"── Gradients ∂out/∂· ──────────────────────────\")\n",
    "print(f\"  ∂/∂w1 = {w1.grad:.4f}   (= x1 = {x1.data})\")\n",
    "print(f\"  ∂/∂w2 = {w2.grad:.4f}   (= x2 = {x2.data})\")\n",
    "print(f\"  ∂/∂b  = {b.grad:.4f}    (= 1)\")\n",
    "print(f\"  ∂/∂x1 = {x1.grad:.4f}   (= w1 = {w1.data})\")\n",
    "print(f\"  ∂/∂x2 = {x2.grad:.4f}  (= w2 = {w2.data})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
